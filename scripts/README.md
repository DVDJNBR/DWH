# üìä Scripts utilitaires

## seed_historical_data.py

Script pour g√©n√©rer des donn√©es historiques dans le Data Warehouse afin de rendre les analyses plus r√©alistes.

### üéØ Pourquoi ?

Quand tu d√©ploies l'infrastructure, le DWH est vide. Les producers g√©n√®rent des donn√©es en temps r√©el, mais tu n'as pas d'historique pour faire des analyses de tendances, des comparaisons mensuelles, etc.

Ce script ins√®re des donn√©es fictives pour les 30 derniers jours (configurable).

### üì¶ Installation

```bash
# Avec uv (recommand√©)
# Les d√©pendances sont g√©r√©es automatiquement via pyproject.toml

# Sur Linux, installer le driver ODBC
sudo apt-get install unixodbc-dev
```

### üöÄ Utilisation

#### M√©thode 1 : Avec les outputs Terraform

```bash
# R√©cup√©rer les infos depuis Terraform
cd terraform
SERVER=$(terraform output -raw sql_server_fqdn)
DATABASE=$(terraform output -raw sql_database_name)

# Lancer le script
cd ..
python scripts/seed_historical_data.py \
    --server $SERVER \
    --database $DATABASE \
    --username dwhadmin \
    --password YourPassword123!
```

#### M√©thode 2 : Manuellement

```bash
# Avec uv (recommand√©)
uv run --directory scripts seed_historical_data.py \
    --server sql-dbreau-whole-rat.database.windows.net \
    --database dwh-shopnow \
    --username dwhadmin \
    --password YourPassword123!

# Ou avec python classique
cd scripts
pip install -e .
python seed_historical_data.py \
    --server sql-dbreau-whole-rat.database.windows.net \
    --database dwh-shopnow \
    --username dwhadmin \
    --password YourPassword123!
```

#### Options avanc√©es

```bash
# G√©n√©rer 60 jours d'historique
python scripts/seed_historical_data.py \
    --server $SERVER \
    --database $DATABASE \
    --username dwhadmin \
    --password YourPassword123! \
    --days 60

# Plus de commandes par jour
python scripts/seed_historical_data.py \
    --server $SERVER \
    --database $DATABASE \
    --username dwhadmin \
    --password YourPassword123! \
    --days 30 \
    --orders-per-day 100 \
    --clicks-per-day 1000
```

### üìä Ce qui est g√©n√©r√©

**Par d√©faut (30 jours)** :

- **100 clients** dans `dim_customer`
- **100 produits** dans `dim_product`
- **1,500 commandes** dans `fact_order` (50/jour √ó 30 jours)
- **15,000 √©v√©nements** dans `fact_clickstream` (500/jour √ó 30 jours)

**Donn√©es r√©alistes** :

- Noms, emails, adresses g√©n√©r√©s avec Faker
- Timestamps r√©partis sur les 30 derniers jours
- Variation des heures (0-23h)
- Mix de statuts (completed, pending, cancelled)
- Mix de types d'√©v√©nements (view_page, add_to_cart, checkout_start)

### üîç V√©rification

Apr√®s l'ex√©cution, le script affiche les statistiques :

```
üìä Statistiques du Data Warehouse:
============================================================
  Clients...................................... 100
  Produits..................................... 100
  Commandes (lignes)........................... 3,750
  √âv√©nements clickstream....................... 15,000

üìÖ P√©riode des commandes:
  Premi√®re commande: 2025-10-25 08:23:15
  Derni√®re commande: 2025-11-24 22:45:32

üìÖ P√©riode des √©v√©nements:
  Premier √©v√©nement: 2025-10-25 00:12:45
  Dernier √©v√©nement: 2025-11-24 23:58:12
============================================================
```

### üé® Analyses possibles apr√®s seeding

Avec des donn√©es historiques, tu peux faire des analyses r√©alistes :

```sql
-- √âvolution des ventes par jour
SELECT 
    CAST(order_timestamp AS DATE) as order_date,
    COUNT(DISTINCT order_id) as orders,
    SUM(quantity * unit_price) as revenue
FROM fact_order
WHERE status = 'completed'
GROUP BY CAST(order_timestamp AS DATE)
ORDER BY order_date;

-- Top produits du mois
SELECT 
    p.name,
    p.category,
    COUNT(*) as times_ordered,
    SUM(f.quantity) as total_quantity,
    SUM(f.quantity * f.unit_price) as revenue
FROM fact_order f
JOIN dim_product p ON f.product_id = p.product_id
WHERE f.order_timestamp >= DATEADD(day, -30, GETDATE())
GROUP BY p.name, p.category
ORDER BY revenue DESC;

-- Taux de conversion (clickstream ‚Üí orders)
WITH daily_stats AS (
    SELECT 
        CAST(event_timestamp AS DATE) as date,
        COUNT(CASE WHEN event_type = 'view_page' THEN 1 END) as views,
        COUNT(CASE WHEN event_type = 'add_to_cart' THEN 1 END) as add_to_carts,
        COUNT(CASE WHEN event_type = 'checkout_start' THEN 1 END) as checkouts
    FROM fact_clickstream
    GROUP BY CAST(event_timestamp AS DATE)
)
SELECT 
    date,
    views,
    add_to_carts,
    checkouts,
    CAST(add_to_carts AS FLOAT) / NULLIF(views, 0) * 100 as cart_rate,
    CAST(checkouts AS FLOAT) / NULLIF(add_to_carts, 0) * 100 as checkout_rate
FROM daily_stats
ORDER BY date;
```

### ‚ö†Ô∏è Notes importantes

1. **Idempotence** : Le script v√©rifie si les clients/produits existent d√©j√† avant insertion
2. **Performance** : Utilise des commits par batch (100 commandes, 500 clics)
3. **Connexion** : N√©cessite que le firewall SQL autorise ton IP
4. **Temps d'ex√©cution** : ~2-3 minutes pour 30 jours de donn√©es

### üîÑ R√©initialisation

Si tu veux repartir de z√©ro :

```sql
-- Vider toutes les tables
TRUNCATE TABLE fact_order;
TRUNCATE TABLE fact_clickstream;
DELETE FROM dim_customer;
DELETE FROM dim_product;
```

Puis relance le script de seeding.

### üöÄ Int√©gration avec Terraform

Tu peux automatiser le seeding apr√®s le d√©ploiement en ajoutant un container qui ex√©cute ce script :

```hcl
resource "azurerm_container_group" "data_seeder" {
  name                = "data-seeder"
  location            = var.location
  resource_group_name = var.resource_group_name
  os_type             = "Linux"
  restart_policy      = "Never"  # Une seule ex√©cution
  
  container {
    name   = "seeder"
    image  = "python:3.12-slim"
    cpu    = 0.5
    memory = 1
    
    commands = [
      "/bin/bash",
      "-c",
      "pip install pyodbc Faker && python seed_historical_data.py ..."
    ]
  }
}
```

### üìö Ressources

- [pyodbc documentation](https://github.com/mkleehammer/pyodbc/wiki)
- [Faker documentation](https://faker.readthedocs.io/)
